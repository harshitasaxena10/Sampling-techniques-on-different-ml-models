# Sampling-techniques-on-different-ml-models
This repository has the code for different sampling techniques applied on 5 different ML models

The following code was written for the assignment given for the subject Predictive analysis and statistics. 
The following .ipynb file has the code for different sampling techniques that are being performed on 5 different ML models, namely Naive Bayes Classifier, Logistic Regression, Decision Tree Classifier, Random Forest and KNN.

Sampling is a technique used to select a particular portion or subset from the population which represents the characteristics of the whole propulation.
Here is the short description of different sampling techniques used in the code:

1) Random sampling: In this technique, the elements of the sample are selected randomly, i.e. each sample has the same probability as other samples to be selected to serve as a representation of an entire population.

2) Stratified sampling: Stratified random sampling is a method of sampling that involves the division of a population into smaller subgroups known as strata. In stratified random sampling, or stratification, the strata are formed based on members’ shared attributes or characteristics

3) Systematic sampling: Systematic sampling is a type of probability sampling method in which sample members from a larger population are selected according to a random starting point but with a fixed, periodic interval. This interval, called the sampling interval, is calculated by dividing the population size by the desired sample size(which can be calculated using mathematical formula).

4) Cluster sampling: The entire population is divided into smaller groups or clusters where they are indicative of homogeneous characteristics and have an equal chance of being a part of the sample.

5) Multi stage sampling: In this technique, cluster sampling was performed, followed by simple random sampling.

Following ML Models are implied in the code:
1) Naive Bayes classifier: Naïve Bayes algorithm is a supervised learning algorithm, which is based on Bayes theorem and used for solving classification problems.It is a probabilistic classifier, which means it predicts on the bsis of probability of an object.
2) Logistic Regression: In logistic regression, a logit transformation is applied on the odds—that is, the probability of success divided by the probability of failure. This is also commonly known as the log odds, or the natural logarithm of odds which is represented by logistic function.
3) Random Forest: Random forest is a machine learning algorithm which combines the output of multiple decision trees to reach a single result. Its ease of use and flexibility have fueled its adoption, as it handles both classification and regression problems.
4) Decision Tree Classifier: This model is a tree structured classifier which has two types of nodes, namely Decision node and leaf node. It is a graphical representation of all possible  solutions.
5) KNN: K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.

CONCLUSION:
We can conclude that from the table of calculated accuracies, random forest has shown maximum accuracy in all of the given sampling techniques.

![image](https://user-images.githubusercontent.com/110947828/219967211-47744cd6-5c52-4a9d-b690-c1a3b8a908e8.png)




